{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Load the processor and model.\n",
    "processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RealSense 파이프라인 시작\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# 색상과 깊이 스트림 활성화 \n",
    "config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 30) \n",
    "config.enable_stream(rs.stream.color, 1920, 1080, rs.format.bgr8, 30)\n",
    "\n",
    "# 파이프라인 시작\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "# 필터 설정\n",
    "spatial = rs.spatial_filter()        # Spatial Edge-Preserving filter\n",
    "#temporal = rs.temporal_filter()      # Temporal filter\n",
    "\n",
    "# Spatial filter 설정\n",
    "spatial.set_option(rs.option.holes_fill, 5)        # Hole filling\n",
    "spatial.set_option(rs.option.filter_smooth_alpha, 0.5)  # Alpha (0 ~ 1)\n",
    "spatial.set_option(rs.option.filter_smooth_delta, 20)   # Delta (1 ~ 50)\n",
    "\n",
    "# depth-to-disparity 변환 필터 추가\n",
    "depth_to_disparity = rs.disparity_transform(True)\n",
    "disparity_to_depth = rs.disparity_transform(False)\n",
    "\n",
    "# Temporal filter 설정\n",
    "#temporal.set_option(rs.option.filter_smooth_alpha, 0.4)  # Alpha (0 ~ 1)\n",
    "\n",
    "# 프레임 세트 수집\n",
    "frameset = pipeline.wait_for_frames()\n",
    "\n",
    "# 깊이 프레임을 기준으로 정렬\n",
    "align = rs.align(rs.stream.color)\n",
    "frameset = align.process(frameset)\n",
    "\n",
    "# 정렬된 컬러 및 깊이 프레임 가져오기\n",
    "aligned_depth_frame = frameset.get_depth_frame()  # 정렬된 깊이 프레임\n",
    "aligned_color_frame = frameset.get_color_frame()  # 정렬된 컬러 프레임\n",
    "\n",
    "# Depth를 Disparity로 변환\n",
    "disparity_frame = depth_to_disparity.process(aligned_depth_frame)\n",
    "\n",
    "# Spatial Filter 적용 (edge-aware)\n",
    "filtered_depth = spatial.process(disparity_frame)\n",
    "\n",
    "\n",
    "# Temporal filter 적용\n",
    "#filtered_depth_frame = temporal.process(filtered_depth_frame)\n",
    "\n",
    "# 다시 Depth로 변환\n",
    "depth_frame_filtered = disparity_to_depth.process(filtered_depth)\n",
    "\n",
    "# 필터가 적용된 깊이 프레임 데이터를 NumPy 배열로 변환\n",
    "final_depth_frame = np.asanyarray(depth_frame_filtered.get_data())\n",
    "\n",
    "# 정렬된 컬러 프레임을 NumPy 배열로 변환\n",
    "aligned_color_image = np.asanyarray(aligned_color_frame.get_data())\n",
    "\n",
    "# 이미지의 밝기 조절 (픽셀 값 증가)\n",
    "# 이미지를 밝게 하려면, 원본 이미지에 일정 값을 더함. 이 예시에서는 50을 더함.\n",
    "#brightness_factor = 50\n",
    "#bright_image = cv2.convertScaleAbs(aligned_color_image, alpha=1, beta=brightness_factor)\n",
    "\n",
    "# PIL로 변환하여 matplotlib을 사용해 시각화\n",
    "blurred_image_rgb = cv2.cvtColor(aligned_color_image, cv2.COLOR_BGR2RGB)  # BGR에서 RGB로 변환\n",
    "inverted_image = Image.fromarray(blurred_image_rgb)\n",
    "\n",
    "# 이미지 표시\n",
    "plt.imshow(inverted_image)\n",
    "plt.show()\n",
    "\n",
    "# 파이프라인 정지\n",
    "pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지와 텍스트 쿼리를 모델에 입력\n",
    "texts = [['box']]  # 원하는 물체에 대한 쿼리\n",
    "inputs = processor(text=texts, images=inverted_image, return_tensors=\"pt\")\n",
    "\n",
    "# OWLv2 모델을 사용하여 객체 감지 수행\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "# 모델 출력 후처리\n",
    "target_sizes = torch.Tensor([inverted_image.size[::-1]])  # 이미지 크기를 사용하여 대상 크기 정의\n",
    "results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "\n",
    "# 결과에서 boxes, scores, labels 추출\n",
    "if results:\n",
    "    boxes, scores, labels = results[0][\"boxes\"], results[0][\"scores\"], results[0][\"labels\"]\n",
    "else:\n",
    "    print(\"No objects detected\")\n",
    "    boxes, scores, labels = [], [], []\n",
    "\n",
    "# 깊이와 픽셀 값 저장할 리스트 초기화\n",
    "depths = []\n",
    "pixels = []  # 픽셀 좌표를 저장할 리스트\n",
    "\n",
    "# 경계 상자와 깊이 정보 추출 및 처리\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    x1, y1, x2, y2 = tuple(box)\n",
    "\n",
    "    # 이미지에 경계 상자 그리기\n",
    "    draw = ImageDraw.Draw(inverted_image)\n",
    "    draw.rectangle(xy=((x1, y1), (x2, y2)), outline=\"red\")\n",
    "    draw.text(xy=(x1, y1), text=texts[0][label], fill='black')\n",
    "\n",
    "    # 경계 상자의 중심 좌표 계산\n",
    "    center_x = (x1 + x2) / 2\n",
    "    center_y = (y1 + y2) / 2\n",
    "\n",
    "    # 중심 좌표에 해당하는 깊이 값 추출\n",
    "    depth_scale = profile.get_device().first_depth_sensor().get_depth_scale()\n",
    "    if 0 <= int(center_y) < final_depth_frame.shape[0] and 0 <= int(center_x) < final_depth_frame.shape[1]: #center_x, center_y가 배열 범위를 벗어나는 경우에 대비해 범위 검사 추가\n",
    "        center_depth_value = final_depth_frame[int(center_y), int(center_x)].astype(float) * depth_scale\n",
    "\n",
    "    # 깊이 값 출력\n",
    "    print(f\"Detected a {texts[0][label]} {center_depth_value:.3f} meters away.\")\n",
    "    \n",
    "    # 중심 좌표와 깊이값을 픽셀 리스트에 추가\n",
    "    bounding_box_pixels = [center_x, center_y, round(center_depth_value, 3)]\n",
    "    pixels.append(bounding_box_pixels)\n",
    "\n",
    "# Deprojection 함수\n",
    "def rs2_deproject_pixel_to_point(intrin, pixels):\n",
    "    points = []\n",
    "    \n",
    "    # 각 픽셀 좌표와 해당 깊이 값을 사용해 Deprojection 수행\n",
    "    for i in range(len(pixels)):\n",
    "        pixel = pixels[i]\n",
    "        depth = pixels[i][2]  # 각 픽셀에 해당하는 깊이 값\n",
    "\n",
    "        x = (pixel[0] - intrin['ppx']) / intrin['fx']\n",
    "        y = (pixel[1] - intrin['ppy']) / intrin['fy']\n",
    "\n",
    "        if intrin['model'] == 'RS2_DISTORTION_INVERSE_BROWN_CONRADY':\n",
    "            r2 = x * x + y * y\n",
    "            f = 1 + intrin['coeffs'][0] * r2 + intrin['coeffs'][1] * r2 * r2 + intrin['coeffs'][4] * r2 * r2 * r2\n",
    "            ux = x * f + 2 * intrin['coeffs'][2] * x * y + intrin['coeffs'][3] * (r2 + 2 * x * x)\n",
    "            uy = y * f + 2 * intrin['coeffs'][3] * x * y + intrin['coeffs'][2] * (r2 + 2 * y * y)\n",
    "            x = ux\n",
    "            y = uy\n",
    "\n",
    "        # Deprojection 결과로 3D 포인트를 구함\n",
    "        point = np.array([depth * x, depth * y, depth])\n",
    "        points.append(point)  # 각 포인트를 리스트에 추가\n",
    "\n",
    "    return points\n",
    "\n",
    "# Example usage\n",
    "#intrin = {\n",
    "#    'model': 'RS2_DISTORTION_INVERSE_BROWN_CONRADY',\n",
    "#    'ppx': 424.9087219238281,\n",
    "#    'ppy': 236.51449584960938,\n",
    "#    'fx': 420.6676330566406,\n",
    "#    'fy': 420.6676330566406,\n",
    "#    'coeffs': [0, 0, 0, 0, 0]\n",
    "#}\n",
    "\n",
    "# Example usage\n",
    "intrin = {\n",
    "    'model': 'RS2_DISTORTION_INVERSE_BROWN_CONRADY',\n",
    "    'ppx': 964.6163330078125,\n",
    "    'ppy': 574.3912963867188,\n",
    "    'fx': 1366.9173583984375,\n",
    "    'fy': 1364.696533203125,\n",
    "    'coeffs': [0, 0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "for i in range(len(pixels)):\n",
    "    \n",
    "    # 각 픽셀 좌표에 대해 Deprojection 수행\n",
    "    points=rs2_deproject_pixel_to_point(intrin, pixels)\n",
    "print(points)\n",
    "\n",
    "plt.imshow(inverted_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rigid body transformation function\n",
    "def apply_rigid_body_transform(point, rotation_matrix, translation_vector):\n",
    "    # Convert point to numpy array if not already\n",
    "    point = np.array(point)\n",
    "\n",
    "    # Apply rigid body transformation: R * point + T\n",
    "    transformed_point = np.dot(R, point) + T\n",
    "    return transformed_point\n",
    "\n",
    "# Example extrinsics from depth to color stream\n",
    "rotation_matrix = [\n",
    "    0.999944269657135, -0.00910048745572567, -0.005356862209737301,\n",
    "    0.009115878492593765, 0.999954342842102, 0.002855878323316574,\n",
    "    0.005330628249794245, -0.0029045515693724155, 0.9999815821647644\n",
    "]\n",
    "\n",
    "# Reshape rotation matrix to 3x3 matrix\n",
    "R = np.array(rotation_matrix).reshape(3, 3) #rotation_matrix를 미리 numpy array로 변환해, 성능에 유리하도록 함.\n",
    "\n",
    "translation_vector = [-0.014955335296690464, -3.531932452460751e-05, -0.00015003549924585968]\n",
    "\n",
    "# Convert translation vector to numpy array\n",
    "T = np.array(translation_vector) #미리 할당된 크기의 numpy 배열로 저장하면 반복문 안에서 메모리 할당이 줄어들어 성능에 유리하게 만들 수 있음.\n",
    "\n",
    "# 리스트 초기화\n",
    "transformed_points = []\n",
    "\n",
    "# 각 3D 포인트에 대해 강체 변환을 적용\n",
    "for point in points:\n",
    "    transformed_point = apply_rigid_body_transform(point, rotation_matrix, translation_vector)\n",
    "    transformed_points.append(transformed_point)\n",
    "    print(\"Transformed 3D Point (in color stream coordinates):\", transformed_point)\n",
    "    print(point)\n",
    "\n",
    "# 출력된 변환된 3D 좌표 리스트\n",
    "print(\"All Transformed Points:\", transformed_points)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
